{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:10:42.334436Z","iopub.execute_input":"2025-09-07T17:10:42.334736Z","iopub.status.idle":"2025-09-07T17:10:42.899660Z","shell.execute_reply.started":"2025-09-07T17:10:42.334710Z","shell.execute_reply":"2025-09-07T17:10:42.898736Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pip install fastapi uvicorn pyngrok transformers accelerate -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:11:42.599476Z","iopub.execute_input":"2025-09-07T17:11:42.600215Z","iopub.status.idle":"2025-09-07T17:12:59.757534Z","shell.execute_reply.started":"2025-09-07T17:11:42.600191Z","shell.execute_reply":"2025-09-07T17:12:59.756679Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:13:33.965446Z","iopub.execute_input":"2025-09-07T17:13:33.966174Z","iopub.status.idle":"2025-09-07T17:13:34.370587Z","shell.execute_reply.started":"2025-09-07T17:13:33.966125Z","shell.execute_reply":"2025-09-07T17:13:34.369825Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4adc60c5f68c47e8a597cb05aaa27de7"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!pip -q install \"transformers>=4.43\" \"accelerate>=0.33\" \"bitsandbytes>=0.43\" \"autoawq>=0.2.7\" torch --extra-index-url https://download.pytorch.org/whl/cu121","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:14:07.987743Z","iopub.execute_input":"2025-09-07T17:14:07.988022Z","iopub.status.idle":"2025-09-07T17:14:17.108802Z","shell.execute_reply.started":"2025-09-07T17:14:07.987999Z","shell.execute_reply":"2025-09-07T17:14:17.108069Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch, os, shutil, pathlib\n\nbase_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\ntok = AutoTokenizer.from_pretrained(base_id, use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:18:29.372779Z","iopub.execute_input":"2025-09-07T17:18:29.373656Z","iopub.status.idle":"2025-09-07T17:18:36.849669Z","shell.execute_reply.started":"2025-09-07T17:18:29.373611Z","shell.execute_reply":"2025-09-07T17:18:36.848800Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/181k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b319f19eb042dfa7209d9cc68d4a6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f117fbca2c7c44e794b0b2dc6b4a2b7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e0cfffa02474ce5b9241b8c4e46c374"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\nmodel = AutoModelForCausalLM.from_pretrained(base_id, quantization_config=bnb_cfg, device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:19:08.803706Z","iopub.execute_input":"2025-09-07T17:19:08.804127Z","iopub.status.idle":"2025-09-07T17:24:06.170372Z","shell.execute_reply.started":"2025-09-07T17:19:08.804104Z","shell.execute_reply":"2025-09-07T17:24:06.169631Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e40f7445d89b49519825885ce74d6ab0"}},"metadata":{}},{"name":"stderr","text":"2025-09-07 17:19:15.047749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757265555.219838      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757265555.268345      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/29.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a1752f444b4d61a9e73abe8ca86ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f539fb54744f4ad19f1ce83aa3f1ff37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267382a31b62428ab97f2ab4e2e54528"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fba733b57014a409a5656b5908fd958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"698ee5b365f64e5fa386ceeab8d169d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d160bead126d4bfab9d74e0ab5e1ccbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c988b82f394ce583651b1713683892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24d0bc44b03a4eedb7d6ca31f4dacb95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa855308d3754c039b39d4aa28978d84"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"save_dir = \"mistral_nemo_instruct_2407_4bit_bnb\"\ntry:\n    model.save_pretrained(save_dir, safe_serialization=True)\n    tok.save_pretrained(save_dir)\n    print(\"Saved quantized model to:\", save_dir)\nexcept Exception as e:\n    print(\"Save 4-bit not supported in this env:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:24:15.211764Z","iopub.execute_input":"2025-09-07T17:24:15.212727Z","iopub.status.idle":"2025-09-07T17:24:41.811881Z","shell.execute_reply.started":"2025-09-07T17:24:15.212698Z","shell.execute_reply":"2025-09-07T17:24:41.809694Z"}},"outputs":[{"name":"stdout","text":"Saved quantized model to: mistral_nemo_instruct_2407_4bit_bnb\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os, time, math, shutil\nfrom pathlib import Path\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nsave_dir = \"/kaggle/working/mistral_nemo_instruct_2407_4bit_bnb\"\n\ndef load_model_and_tokenizer(path_or_id):\n    bnb_cfg = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n    )\n    tok = AutoTokenizer.from_pretrained(path_or_id, use_fast=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        path_or_id,\n        quantization_config=bnb_cfg,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n    )\n    return tok, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:25:37.682626Z","iopub.execute_input":"2025-09-07T17:25:37.683224Z","iopub.status.idle":"2025-09-07T17:25:37.689193Z","shell.execute_reply.started":"2025-09-07T17:25:37.683198Z","shell.execute_reply":"2025-09-07T17:25:37.688374Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"if Path(save_dir).exists():\n    print(f\"Loading local 4-bit checkpoint: {save_dir}\")\n    tok, model = load_model_and_tokenizer(save_dir)\nelse:\n    print(\"Local 4-bit checkpoint not found (saving likely not supported in this env).\")\n    print(f\"Falling back to: {fallback_hub_id}\")\n    tok, model = load_model_and_tokenizer(fallback_hub_id)\n\ndef generate(prompt, max_new_tokens=128, temperature=0.7, top_p=0.95):\n    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n    t0 = time.time()\n    out = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=top_p,\n        pad_token_id=tok.eos_token_id,\n    )\n    t1 = time.time()\n    text = tok.decode(out[0], skip_special_tokens=True)\n    print(f\"\\n--- Generation ({t1 - t0:.2f}s) ---\\n{text}\\n\")\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:25:46.402566Z","iopub.execute_input":"2025-09-07T17:25:46.402912Z","iopub.status.idle":"2025-09-07T17:25:53.589070Z","shell.execute_reply.started":"2025-09-07T17:25:46.402890Z","shell.execute_reply":"2025-09-07T17:25:53.588124Z"}},"outputs":[{"name":"stdout","text":"Loading local 4-bit checkpoint: /kaggle/working/mistral_nemo_instruct_2407_4bit_bnb\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e891d7f0e6484de8a1c975c7dfa37557"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"_ = generate(\"Explain model quantization to students in two sentences.\")\n_ = generate(\"Give three bullet points comparing fp16 vs int8 vs 4-bit quantization.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:26:04.041857Z","iopub.execute_input":"2025-09-07T17:26:04.042138Z","iopub.status.idle":"2025-09-07T17:26:28.297869Z","shell.execute_reply.started":"2025-09-07T17:26:04.042117Z","shell.execute_reply":"2025-09-07T17:26:28.296960Z"}},"outputs":[{"name":"stdout","text":"\n--- Generation (9.73s) ---\nExplain model quantization to students in two sentences.Quantization is a technique used in machine learning model optimization that reduces the precision of weights and activations in a model, typically from float32 to int8 or float16, without significantly sacrificing model accuracy. This process not only reduces the model's memory footprint but also speeds up inference time, making it more efficient for deployment on resource-limited devices.\n\n\n--- Generation (14.52s) ---\nGive three bullet points comparing fp16 vs int8 vs 4-bit quantization.  \n- **fp16**:\n  - **Precision**: fp16 has half the number of bits compared to fp32, offering reduced precision but retaining some flexibility for complex operations.\n  - **Memory Efficiency**: fp16 saves half the memory compared to fp32, but it's less efficient than int8 and 4-bit quantization.\n  - **Speed**: fp16 operations are typically faster than fp32 on hardware supporting it, like Nvidia GPUs.\n\n- **int8**:\n  - **Precision**: int8 offers fixed-point precision with limited dynamic range, suitable for simpler neural network operations.\n \n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"NGROK_TOKEN = \"32NdJS7skdlVn1em50DJ77sUiju_AFPKSCuPnJcxvND2XVY3\"\nAPI_KEY = \"secret123\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:27:58.890356Z","iopub.execute_input":"2025-09-07T17:27:58.891273Z","iopub.status.idle":"2025-09-07T17:27:58.895262Z","shell.execute_reply.started":"2025-09-07T17:27:58.891236Z","shell.execute_reply":"2025-09-07T17:27:58.894354Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from fastapi import FastAPI, Request, HTTPException\nimport uvicorn, threading, time, socket\nfrom pyngrok import ngrok, conf\n\napp = FastAPI()\n\n@app.post(\"/generate\")\nasync def gen(req: Request):\n    if req.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n    data = await req.json()\n    return {\n        \"response\": generate(\n            data.get(\"prompt\", \"\"),\n            data.get(\"max_length\", 300)\n        )\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:51:53.350753Z","iopub.execute_input":"2025-09-07T17:51:53.351081Z","iopub.status.idle":"2025-09-07T17:51:53.357166Z","shell.execute_reply.started":"2025-09-07T17:51:53.351057Z","shell.execute_reply":"2025-09-07T17:51:53.356345Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def free_port():\n    s = socket.socket()\n    s.bind(('', 0))\n    port = s.getsockname()[1]\n    s.close()\n    return port\n\nport = free_port()\nconf.get_default().auth_token = NGROK_TOKEN\npublic_url = ngrok.connect(port).public_url\nprint(\"Your public URL:\", public_url)\n\ndef run(): uvicorn.run(app, host=\"0.0.0.0\", port=port)\nthreading.Thread(target=run, daemon=True).start()\ntime.sleep(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:51:57.634823Z","iopub.execute_input":"2025-09-07T17:51:57.635614Z","iopub.status.idle":"2025-09-07T17:51:58.674920Z","shell.execute_reply.started":"2025-09-07T17:51:57.635580Z","shell.execute_reply":"2025-09-07T17:51:58.674090Z"}},"outputs":[{"name":"stdout","text":"Your public URL: https://017f01129768.ngrok-free.app\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [36]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:60737 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"\n--- Generation (31.09s) ---\nwho is CR7? Cristiano Ronaldo dos Santos Aveiro Ronaldo Jr., GOIH, commonly known as Cristiano Ronaldo, is a Portuguese professional footballer who plays as a forward for Saudi Arabian club Al Nassr and captains the Portugal national team.He is often considered the best player in the world and has repeatedly been ranked first or second best player in the world in various publications.\n\nRonaldo holds several records and accolades throughout his career. Here are some highlights:\n\n1. **Club Career:**\n   - He played for Manchester United from 2003 to 2009, winning three Premier League titles and the UEFA Champions League in 2008.\n   - From 2009 to 2018, he played for Real Madrid, winning two La Liga titles, two Copa del Rey titles, and four UEFA Champions League titles.\n   - Since 2018, he has played for Juventus, winning two Serie A titles and the Coppa Italia in his first season.\n   - Currently, he plays for Al Nassr in Saudi Arabia.\n\n2. **International Career:**\n   - Ronaldo captains the Portugal national team. He led them to victory in the UEFA Euro 2016 and UEFA Nations League in 2019.\n   - He is Portugal's most capped player and all-time top scorer.\n\n3. **Individual Awards:**\n   - Ronaldo has won five Ballon d'Or awards (2008, 2013\n\nINFO:     51.39.103.236:0 - \"POST /generate HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":15}]}